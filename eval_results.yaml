- dataset:
    id: cais/hle                  # Required. A valid dataset id from the Hub, which should be a Benchmark.
                                  # ^Basically, where does the leaderboard live.
    config: {dataset_config}      # Optional. The name of the dataset subset or config (when it has different subsets)
    split: {dataset_split}        # Optional. Example: test, validation
    revision: {dataset_revision}  # Optional. Example: 5503434ddd753f426f4b38109466949a1217c2bb

  value: {metric_value}       # Required. Example: 20.90

  verifyToken: {verify_token} # Optional. If present, this is a signature that can be used to prove that evaluation is provably auditable and reproducible. 
                              # (For example, was run in a HF Job using inspect-ai or lighteval)

  date: {date}      # Optional. When was this eval run (ISO-8601 datetime). If not provided, can default to this file creation time in git.

  source:                         # Optional. The source for this result, for instance a dataset repo.
    url: {source_url}             # Required if source is provided. A link to the source. Example: https://huggingface.co/spaces/SaylorTwift/smollm3-mmlu-pro.
    name: {source_name}           # Optional. The name of the source. Example: Eval Logs.
    user: {username}              # Optional. A HF user or org name.

# or, with only the required attributes:

- dataset:
    id: Idavidrein/gpqa
    config: gpqa_diamond
  value: 0.412

