{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLMs with TRL CLI on SageMaker\n",
    "\n",
    "This notebook shows how to fine-tune language models on AWS SageMaker using the **`trl sft` CLI** - the same command used by [trl-jobs](https://github.com/huggingface/trl-jobs) on HuggingFace.\n",
    "\n",
    "**Hardware:** `ml.g6e.12xlarge` (4x L40S) or `ml.p4de.24xlarge` (8x A100)\n",
    "\n",
    "**Why TRL CLI?**\n",
    "- Zero Python code - just config and run\n",
    "- Multi-GPU training via `accelerate`\n",
    "- Compatible with [trl-jobs configs](https://github.com/huggingface/trl-jobs/tree/main/trl_jobs/configs)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS credentials configured\n",
    "- SageMaker execution role with S3 access\n",
    "- HuggingFace token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2699\ufe0f Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/pytorch/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/pytorch/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/pytorch/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/pytorch/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/pytorch/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ackaging (/opt/pytorch/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd6c1a186dd4e96938fa82a27e2ea2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN set: hf_MICph...\n"
     ]
    }
   ],
   "source": [
    "# Authenticate with Hugging Face\n",
    "import os\n",
    "from huggingface_hub import login, get_token\n",
    "\n",
    "login()\n",
    "\n",
    "HF_TOKEN = get_token()\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "print(f\"HF_TOKEN set: {HF_TOKEN[:8]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.train.model_trainer import ModelTrainer\n",
    "from sagemaker.train.configs import SourceCode, Compute, StoppingCondition, OutputDataConfig\n",
    "from sagemaker.core.helper.session_helper import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Account: 754289655784\n",
      "Role: arn:aws:iam::754289655784:role/sagemaker-dlcs\n"
     ]
    }
   ],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = Session()\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='sagemaker-dlcs')['Role']['Arn']\n",
    "region = sagemaker_session.boto_region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Account: {account_id}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2699\ufe0f Configuration\n",
    "\n",
    "We use **YAML config files** exactly like [trl-jobs](https://github.com/huggingface/trl-jobs/blob/main/trl_jobs/configs/Qwen3-4B-a100-large.yaml).\n",
    "\n",
    "**Hardware:** 4x L40S GPUs on `ml.g6e.12xlarge` (192GB total VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen3-4B-Instruct-2507\n",
      "Dataset: OpenMed/Medical-Reasoning-SFT-GPT-OSS-120B\n",
      "Instance: ml.p4de.24xlarge (8x L40S GPUs)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL & INSTANCE CONFIGURATION\n",
    "# =============================================================================\n",
    "# Based on trl-jobs optimal configs:\n",
    "# https://github.com/huggingface/trl-jobs/blob/main/trl_jobs/configs/\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"  # trl-jobs optimal config\n",
    "DATASET_NAME = \"OpenMed/Medical-Reasoning-SFT-GPT-OSS-120B\"\n",
    "\n",
    "# Instance: ml.g6e.12xlarge = 4x L40S GPUs (48GB each = 192GB total)\n",
    "# Note: g6e.16xlarge only has 1 GPU, g6e.12xlarge/g6e.24xlarge have 4 GPUs\n",
    "INSTANCE_TYPE = \"ml.p4de.24xlarge\"\n",
    "NUM_GPUS = 8\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Instance: {INSTANCE_TYPE} ({NUM_GPUS}x L40S GPUs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output model name: Qwen3-4B-Base-SFT-20260120162518\n",
      "(Copy this to hub_model_id and run_name in the YAML config cell)\n"
     ]
    }
   ],
   "source": [
    "# Generate unique output model name for hub_model_id and run_name\n",
    "timestamp = time.strftime(\"%Y%m%d%H%M%S\", time.gmtime())\n",
    "OUTPUT_MODEL_NAME = f\"Qwen3-4B-Base-SFT-{timestamp}\"\n",
    "\n",
    "print(f\"Output model name: {OUTPUT_MODEL_NAME}\")\n",
    "print(\"(Copy this to hub_model_id and run_name in the YAML config cell)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 TRL CLI with YAML Config\n",
    "\n",
    "TRL CLI has **native accelerate support** - just put `num_processes` in the YAML config:\n",
    "\n",
    "```bash\n",
    "trl sft --config sft_config.yaml\n",
    "```\n",
    "\n",
    "No need for `accelerate launch` wrapper! See [TRL CLI docs](https://huggingface.co/docs/trl/en/clis#scaling-up-with-accelerate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command:\n",
      "  trl sft --config sft_config.yaml\n",
      "\n",
      "Config: scripts/sft_config.yaml\n",
      "Model: Qwen/Qwen3-4B-Base\n",
      "Dataset: trl-lib/Capybara\n",
      "GPUs: 4 (via num_processes in YAML)\n"
     ]
    }
   ],
   "source": [
    "# Preview the command that will be executed\n",
    "print(\"Command:\")\n",
    "print(\"  trl sft --config sft_config.yaml\")\n",
    "print()\n",
    "print(\"Config: scripts/sft_config.yaml\")\n",
    "print(\"Model: Qwen/Qwen3-4B-Base\")\n",
    "print(\"Dataset: trl-lib/Capybara\")\n",
    "print(\"GPUs: 4 (via num_processes in YAML)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd Create Training Files\n\nWe create:\n1. **`sft_config.yaml`** - TRL SFT config (same format as trl-jobs)\n2. **`run_sft.py`** - Entry script that calls `trl sft --config`\n3. **`requirements.txt`** - Python dependencies\n\n### \ud83d\udcca Training Logging with Trackio\n\nThis notebook uses [Trackio](https://huggingface.co/docs/trl/en/trackio_integration) for real-time training metrics visualization. Trackio is TRL's native integration for logging training runs to a Hugging Face Space.\n\n**To use Trackio:**\n1. Set `report_to: trackio` in your config\n2. Configure `TRACKIO_SPACE_ID` (your HF Space) and `TRACKIO_PROJECT` (project name) as environment variables\n\n**To disable:** Change `report_to: trackio` to `report_to: none` in the config and remove the `TRACKIO_*` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Created scripts/\n"
     ]
    }
   ],
   "source": [
    "# Create scripts directory\n",
    "import shutil\n",
    "\n",
    "source_dir = Path(\"scripts\")\n",
    "if source_dir.exists():\n",
    "    shutil.rmtree(source_dir)\n",
    "source_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\u2713 Created {source_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/run_sft.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/run_sft.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Entry script for TRL SFT training on SageMaker.\n",
    "\n",
    "Runs: trl sft --config sft_config.yaml\n",
    "\n",
    "TRL CLI natively supports accelerate - num_processes is in the YAML config.\n",
    "See: https://huggingface.co/docs/trl/en/clis#scaling-up-with-accelerate\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRL SFT Training on SageMaker\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Environment info\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "    print(f\"HF_TOKEN: {hf_token[:8]}...\" if hf_token else \"HF_TOKEN: NOT SET\")\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "    # Find config file\n",
    "    config_path = Path(\"/opt/ml/input/data/code/sft_config.yaml\")\n",
    "    if not config_path.exists():\n",
    "        config_path = Path(\"sft_config.yaml\")\n",
    "\n",
    "    print(f\"Config file: {config_path}\")\n",
    "    print()\n",
    "\n",
    "    # Print config contents\n",
    "    print(\"Configuration:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(config_path.read_text())\n",
    "    print(\"-\" * 40)\n",
    "    print()\n",
    "\n",
    "    # TRL CLI has native accelerate support - just pass the config\n",
    "    # num_processes, mixed_precision, etc. are read from the YAML\n",
    "    cmd = [\"trl\", \"sft\", \"--config\", str(config_path)]\n",
    "\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Execute\n",
    "    result = subprocess.run(cmd, check=False)\n",
    "    sys.exit(result.returncode)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/requirements.txt\n",
    "trl>=0.12.0\n",
    "transformers>=4.45.0\n",
    "datasets>=3.0.0\n",
    "peft>=0.13.0\n",
    "accelerate>=1.0.0\n",
    "huggingface_hub>=0.26.0\n",
    "liger-kernel>=0.4.0\n",
    "flash-attn>=2.0.0\n",
    "trackio\n",
    "hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/sft_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/sft_config.yaml\n",
    "# TRL SFT Config - Based on trl-jobs\n",
    "# https://github.com/huggingface/trl-jobs/blob/main/trl_jobs/configs/Qwen3-4B-a100-large.yaml\n",
    "# Adapted for 4x L40S GPUs (ml.g6e.12xlarge)\n",
    "\n",
    "# Accelerate arguments\n",
    "num_processes: 8\n",
    "num_machines: 1\n",
    "mixed_precision: \"no\"\n",
    "dynamo_backend: \"no\"\n",
    "\n",
    "# Model arguments\n",
    "model_name_or_path: Qwen/Qwen3-4B-Instruct-2507\n",
    "model_revision: main\n",
    "torch_dtype: bfloat16\n",
    "attn_implementation: kernels-community/flash-attn2\n",
    "use_peft: true\n",
    "\n",
    "# Data arguments\n",
    "dataset_name: OpenMed/Medical-Reasoning-SFT-GPT-OSS-120B\n",
    "dataset_num_proc: 96\n",
    "\n",
    "# Training arguments\n",
    "bf16: true\n",
    "do_eval: false\n",
    "eval_strategy: \"no\"\n",
    "gradient_accumulation_steps: 8\n",
    "gradient_checkpointing: true\n",
    "gradient_checkpointing_kwargs:\n",
    "  use_reentrant: false\n",
    "learning_rate: 2.0e-04\n",
    "log_level: info\n",
    "logging_strategy: steps\n",
    "logging_steps: 10\n",
    "lr_scheduler_type: cosine_with_min_lr\n",
    "lr_scheduler_kwargs:\n",
    "  min_lr_rate: 0.1\n",
    "max_grad_norm: 0.2\n",
    "max_length: 24576\n",
    "num_train_epochs: 1\n",
    "output_dir: /opt/ml/model\n",
    "overwrite_output_dir: true\n",
    "packing: true\n",
    "pad_to_multiple_of: 4096\n",
    "per_device_eval_batch_size: 1\n",
    "per_device_train_batch_size: 1\n",
    "seed: 42\n",
    "use_liger_kernel: true\n",
    "warmup_ratio: 0.03\n",
    "\n",
    "# Checkpointing\n",
    "save_steps: 100\n",
    "save_strategy: steps\n",
    "save_total_limit: 2\n",
    "\n",
    "# Hub & Logging\n",
    "push_to_hub: true\n",
    "hub_strategy: every_save\n",
    "hub_model_id: Qwen3-4B-Instruct-OpenMed  # <-- UPDATE with OUTPUT_MODEL_NAME from Cell 10\n",
    "run_name: Qwen3-4B-Instruct-OpenMed-SFT      # <-- UPDATE with OUTPUT_MODEL_NAME from Cell 10\n",
    "report_to: trackio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of scripts/:\n",
      "  \u2713 requirements.txt\n",
      "  \u2713 run_sft.py\n",
      "  \u2713 sft_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Verify all files created\n",
    "print(\"Contents of scripts/:\")\n",
    "for f in sorted(source_dir.iterdir()):\n",
    "    print(f\"  \u2713 {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Launch Training Job\n",
    "\n",
    "We use the SageMaker `ModelTrainer` with `SourceCode` to launch the training job.\n",
    "\n",
    "The TRL CLI arguments are embedded directly in the entry script to avoid SageMaker/accelerate argument parsing conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/20/26 16:28:31] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> OutputDataConfig compression type not provided. Using default:         <a href=\"file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/defaults.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">defaults.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/defaults.py#162\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         GZIP                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/20/26 16:28:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m OutputDataConfig compression type not provided. Using default:         \u001b]8;id=547587;file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/defaults.py\u001b\\\u001b[2mdefaults.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=67074;file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/defaults.py#162\u001b\\\u001b[2m162\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         GZIP                                                                   \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Training image URI:                                               <a href=\"file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/model_trainer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_trainer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/model_trainer.py#537\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">537</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">763104351884.</span>dkr.ecr.us-east-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>amazonaws.com/huggingface-pytorch- <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         training:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.8</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-transformers4.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56.2</span>-gpu-py312-cu129-ubuntu22.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-v1. <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Training image URI:                                               \u001b]8;id=604457;file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/model_trainer.py\u001b\\\u001b[2mmodel_trainer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=914028;file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/model_trainer.py#537\u001b\\\u001b[2m537\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1;36m763104351884.\u001b[0mdkr.ecr.us-east-\u001b[1;36m1.\u001b[0mamazonaws.com/huggingface-pytorch- \u001b[2m                    \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         training:\u001b[1;36m2.8\u001b[0m.\u001b[1;36m0\u001b[0m-transformers4.\u001b[1;36m56.2\u001b[0m-gpu-py312-cu129-ubuntu22.\u001b[1;36m04\u001b[0m-v1. \u001b[2m                    \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1;36m2\u001b[0m                                                                 \u001b[2m                    \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name: trl-sft-yaml-97d3b6c5\n",
      "Instance: ml.p4de.24xlarge (8x L40S GPUs)\n",
      "Output: s3://sagemaker-us-east-1-754289655784/trl-sft-yaml\n",
      "Image: huggingface-pytorch-training:2.8.0-transformers4.56.2-gpu-py312-cu129-ubuntu22.04-v1.2\n",
      "\n",
      "Ready to launch!\n"
     ]
    }
   ],
   "source": [
    "# Job configuration\n",
    "PROJECT_NAME = \"trl-sft-yaml\"\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "S3_OUTPUT_URI = f\"s3://{BUCKET_NAME}/{PROJECT_NAME}\"\n",
    "\n",
    "unique_id = uuid.uuid4().hex[:8]\n",
    "base_job_name = f\"{PROJECT_NAME}-{unique_id}\"\n",
    "\n",
    "# Training image - PyTorch DLC with GPU support\n",
    "TRAINING_IMAGE = f\"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:2.8.0-transformers4.56.2-gpu-py312-cu129-ubuntu22.04-v1.2\"\n",
    "\n",
    "# Create ModelTrainer with SourceCode\n",
    "trainer = ModelTrainer(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    training_mode=\"SAGEMAKER_TRAINING_JOB\",\n",
    "    source_code=SourceCode(\n",
    "        source_dir=str(source_dir),\n",
    "        entry_script=\"run_sft.py\",\n",
    "        requirements=\"requirements.txt\",\n",
    "    ),\n",
    "    compute=Compute(\n",
    "        instance_type=INSTANCE_TYPE,\n",
    "        instance_count=1,\n",
    "        volume_size_in_gb=200,  # Larger for 4B model + checkpoints\n",
    "    ),\n",
    "    stopping_condition=StoppingCondition(\n",
    "        max_runtime_in_seconds=6 * 60 * 60,  # 6 hours\n",
    "    ),\n",
    "    output_data_config=OutputDataConfig(\n",
    "        s3_output_path=f\"{S3_OUTPUT_URI}/output/\",\n",
    "    ),\n",
    "    base_job_name=base_job_name,\n",
    "    environment={\n",
    "        \"HF_TOKEN\": HF_TOKEN,\n",
    "        \"TRANSFORMERS_VERBOSITY\": \"info\",\n",
    "        \"NCCL_DEBUG\": \"WARN\",  # For multi-GPU debugging\n",
    "        # Trackio logging - https://huggingface.co/docs/trl/en/trackio_integration\n",
    "        \"TRACKIO_SPACE_ID\": \"florentgbelidji/trackio\",\n",
    "        \"TRACKIO_PROJECT\": \"trl-sft\",\n",
    "        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n",
    "    },\n",
    "    training_image=TRAINING_IMAGE,\n",
    ")\n",
    "\n",
    "print(f\"Job name: {base_job_name}\")\n",
    "print(f\"Instance: {INSTANCE_TYPE} ({NUM_GPUS}x L40S GPUs)\")\n",
    "print(f\"Output: {S3_OUTPUT_URI}\")\n",
    "print(f\"Image: {TRAINING_IMAGE.split('/')[-1]}\")\n",
    "print(f\"\\nReady to launch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training job: trl-sft-yaml-97d3b6c5\n",
      "Model: Qwen/Qwen3-4B-Instruct-2507\n",
      "Dataset: OpenMed/Medical-Reasoning-SFT-GPT-OSS-120B\n",
      "Instance: ml.p4de.24xlarge (8x L40S)\n",
      "\n",
      "Using: trl sft --config sft_config.yaml\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/20/26 16:29:57] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/core/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/core/telemetry/telemetry_logging.py#92\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">92</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/20/26 16:29:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=538013;file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/core/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=87043;file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/core/telemetry/telemetry_logging.py#92\u001b\\\u001b[2m92\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training_job resource.                                     <a href=\"file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/core/resources.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">resources.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/core/resources.py#35539\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35539</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training_job resource.                                     \u001b]8;id=465617;file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/core/resources.py\u001b\\\u001b[2mresources.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=937542;file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/core/resources.py#35539\u001b\\\u001b[2m35539\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/20/26 16:29:58] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> Not displaing the training container logs as <span style=\"color: #008700; text-decoration-color: #008700\">'wait'</span> is set to     <a href=\"file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/model_trainer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_trainer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/model_trainer.py#762\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">762</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #d70000; text-decoration-color: #d70000; font-style: italic\">False</span>.                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/20/26 16:29:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m Not displaing the training container logs as \u001b[38;2;0;135;0m'wait'\u001b[0m is set to     \u001b]8;id=344544;file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/model_trainer.py\u001b\\\u001b[2mmodel_trainer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=644215;file:///opt/pytorch/lib/python3.12/site-packages/sagemaker/train/model_trainer.py#762\u001b\\\u001b[2m762\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[3;38;2;215;0;0mFalse\u001b[0m.                                                            \u001b[2m                    \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch the training job\n",
    "print(f\"Launching training job: {base_job_name}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Instance: {INSTANCE_TYPE} ({NUM_GPUS}x L40S)\")\n",
    "print(f\"\\nUsing: trl sft --config sft_config.yaml\")\n",
    "print()\n",
    "\n",
    "# Set wait=True to stream logs, wait=False to run in background\n",
    "trainer.train(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Monitor Training\n",
    "\n",
    "If you launched with `wait=False`, you can monitor the job here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name: trl-sft-yaml-dfe9c258\n",
      "Console: https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/trl-sft-yaml-dfe9c258\n"
     ]
    }
   ],
   "source": [
    "# Get the job name\n",
    "training_job_name = trainer.latest_training_job.name if hasattr(trainer, 'latest_training_job') else base_job_name\n",
    "print(f\"Job name: {training_job_name}\")\n",
    "\n",
    "# View in console\n",
    "console_url = f\"https://{region}.console.aws.amazon.com/sagemaker/home?region={region}#/jobs/{training_job_name}\"\n",
    "print(f\"Console: {console_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Load and Test the Model\n",
    "\n",
    "After training completes, the model is pushed to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model from Hub\n",
    "from transformers import pipeline\n",
    "\n",
    "# Your HF username\n",
    "HF_USERNAME = \"florentgbelidji\"  # Change this!\n",
    "\n",
    "finetuned_model_id = f\"{HF_USERNAME}/{OUTPUT_MODEL_NAME}\"\n",
    "print(f\"Model will be available at: {finetuned_model_id}\")\n",
    "\n",
    "# Uncomment after training completes\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=finetuned_model_id,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "# output = pipe(messages, max_new_tokens=100)\n",
    "# print(output[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddf9 Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(source_dir, ignore_errors=True)\n",
    "print(\"Cleaned up temporary files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}