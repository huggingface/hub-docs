# ðŸ¤— Hugging Face Inference Providers for VS Code Copilot

![Demo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/demo.gif)

You can now use SoTA openâ€‘source LLMs in VS Code via Copilot Chat with [Hugging Face Inference Providers](https://huggingface.co/docs/inference-providers/index) ðŸ”¥

## âš¡ Quick start
1. Install the HF Copilot Chat extension [here](#todo).
1. Open VS Code's chat interface.
2. Click the model picker and click "Manage Models...".
3. Select "Hugging Face" provider.
4. Provide your Hugging Face Token, you can get one in your [settings page](https://huggingface.co/settings/tokens/new?ownUserPermissions=inference.serverless.write&tokenType=fineGrained).
5. Select the models you want to add to the model picker.

## âœ¨ Why use the Hugging Face provider in Copilot
- 4k+ openâ€‘source LLMs with tool calling capabilities.
- Single API to thousands of openâ€‘source LLMs via providers like Groq, Cerebras, Together AI, SambaNova, and more.
- Built for high availability (across providers) and low latency through worldâ€‘class providers.
- No extra markup on provider rates.

ðŸ’¡ The free tier gives you a small amount of monthly inference credits to experiment. Upgrade to [Hugging Face PRO](https://huggingface.co/pro) or [Enterprise](https://huggingface/enterprise) for $2 in monthly credits plus payâ€‘asâ€‘youâ€‘go access across all providers!