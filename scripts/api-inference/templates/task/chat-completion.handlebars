## Chat Completion

Generate a response given a list of messages in a conversational context, supporting both conversational Language Models (LLMs) and conversational Vision-Language Models (VLMs).
This is a subtask of [`text-generation`](https://huggingface.co/docs/api-inference/tasks/text-generation) and [`image-text-to-text`](https://huggingface.co/docs/api-inference/tasks/image-text-to-text).

### Recommended models

#### Conversational Large Language Models (LLMs)

{{#each models.chat-completion}}
- [{{this.id}}](https://huggingface.co/{{this.id}}): {{this.description}}
{{/each}}

#### Conversational Vision-Language Models (VLMs)

{{#each models.conversational-image-text-to-text}}
- [{{this.id}}](https://huggingface.co/{{this.id}}): {{this.description}}
{{/each}}

### Using the API

The API supports:

* Using the chat completion API compatible with the OpenAI SDK.
* Using grammars, constraints, and tools.
* Streaming the output

#### Code snippet example for conversational LLMs

{{{snippets.chat-completion}}}

#### Code snippet example for conversational VLMs

{{{snippets.conversational-image-text-to-text}}}

### API specification

#### Request

{{{specs.chat-completion.input}}}

{{{constants.specsHeaders}}}

#### Response

Output type depends on the `stream` input parameter.
If `stream` is `false` (default), the response will be a JSON object with the following fields:

{{{specs.chat-completion.output}}}

If `stream` is `true`, generated tokens are returned as a stream, using Server-Sent Events (SSE).
For more information about streaming, check out [this guide](https://huggingface.co/docs/text-generation-inference/conceptual/streaming).

{{{specs.chat-completion.stream_output}}}

