##Â Use Cases
Video classification models can be used to categorize what a video is all about.

### Activity Recognition
Video classification models are used to perform activity recognition which is useful for fitness applications. Activity  recognition is also helpful for vision-impaired individuals especially when they're commuting.

### Video Search
Models trained in video classification can improve user experience by organizing and categorizing video galleries on the phone or in the cloud, on multiple keywords or tags.

## Inference

Below you can find code for inferring with a pre-trained video classification model.

```python
from decord import VideoReader, cpu
import torch
import numpy as np

from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification
from huggingface_hub import hf_hub_download

np.random.seed(0)


# Videos consist of multiple frames which are static images. Below is a 
# utility to sample a few frames from the input video.
def sample_frame_indices(clip_len, frame_sample_rate, seg_len):
    converted_len = int(clip_len * frame_sample_rate)
    end_idx = np.random.randint(converted_len, seg_len)
    start_idx = end_idx - converted_len
    indices = np.linspace(start_idx, end_idx, num=clip_len)
    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)
    return indices

# Download a video clip for inference.
# video clip consists of 300 frames (10 seconds at 30 FPS)
file_path = hf_hub_download(
    repo_id="nielsr/video-demo", filename="eating_spaghetti.mp4", repo_type="dataset"
)
# We initialize a VideoReader to stream the video frame by frame.
videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))

# Sample 16 frames and collate them in a batch.
videoreader.seek(0)
indices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))
video = videoreader.get_batch(indices).asnumpy()

# load the feature extractor to preprocess the video frames and the video
# classification model
feature_extractor = VideoMAEFeatureExtractor.from_pretrained("MCG-NJU/videomae-base-finetuned-kinetics")
model = VideoMAEForVideoClassification.from_pretrained("MCG-NJU/videomae-base-finetuned-kinetics")

# preprocess the input frames
inputs = feature_extractor(list(video), return_tensors="pt")

# run inference
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# model predicts one of the 400 Kinetics 400 classes
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])
# eating spaghetti
```

## Useful Resources

- [Developing a simple video classification model](https://keras.io/examples/vision/video_classification)
- [Video classification with Transformers](https://keras.io/examples/vision/video_transformers)
- [Building a video archive](https://www.youtube.com/watch?v=_IeS1m8r6SY)

### Creating your own video classifier in minutes
- [Fine-tuning tutorial notebook (PyTorch)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/video_classification.ipynb)
